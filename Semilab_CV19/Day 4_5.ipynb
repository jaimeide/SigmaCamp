{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Days 4 & 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "- Understand key concepts (15 min)\n",
    "- Face detection (30 min)\n",
    "- Face landmark detection (15 min)\n",
    "- Opening and handling videos (EXTRA)\n",
    "- [  ] Run tracking live demo (15 min)\n",
    "- Create a face recognition (60 min)\n",
    "- [  ] Run object detection and tracking live demo (15 min)\n",
    "- Conclusions (15 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open your images\n",
    "# Note: face detectors work in grayscale (*)\n",
    "semilab = cv2.imread('semilab.jpg',0) # choose any pic that has faces\n",
    "mylab = cv2.imread('lab2005.jpg',0) # put the name of the group pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(semilab,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mylab,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a classifier with given \"cascades\" (set of features, kernels)\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function takes an image as input, find all the faces and\n",
    "#   draw rectangles around them.\n",
    "def detect_face(img):\n",
    "    # Copy matrix\n",
    "    face_img = img.copy()\n",
    "    \n",
    "    # Built-in function for face detection\n",
    "    face_rects = face_cascade.detectMultiScale(face_img) \n",
    "    # You could change the default parameters...\n",
    "    #face_rects = face_cascade.detectMultiScale(face_img,scaleFactor=1.2, minNeighbors=5)\n",
    "    \n",
    "    # Draw faces  (previous function returns the rectangle coordinate)\n",
    "    for (x,y,w,h) in face_rects: \n",
    "        cv2.rectangle(face_img, (x,y), (x+w,y+h), (255,255,255), 5) \n",
    "        \n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the face detection\n",
    "res = detect_face(semilab)\n",
    "plt.imshow(res,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running the same algorithm after equalizing the image\n",
    "res = detect_face(cv2.equalizeHist(semilab))\n",
    "plt.imshow(res,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye Detection with Haar-like cascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a face image\n",
    "face = cv2.imread('ji3.jpg',0)\n",
    "plt.imshow(face,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the eye detector\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the eye detector function\n",
    "def detect_eyes(img):\n",
    "    # Copy image\n",
    "    face_img = img.copy()\n",
    "    # Detect all the eyes (return the rectangles)\n",
    "    eyes = eye_cascade.detectMultiScale(face_img) \n",
    "    # Draw the boxes\n",
    "    for (x,y,w,h) in eyes: \n",
    "        cv2.rectangle(face_img, (x,y), (x+w,y+h), (255,255,255), 2) \n",
    "    return face_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run detector after equalizing the image\n",
    "res = detect_eyes(cv2.equalizeHist(face))\n",
    "plt.imshow(res,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Connects to your computer's default camera\n",
    "cap = cv2.VideoCapture(0) # 0 is for default camera\n",
    "\n",
    "# Automatically grab width and height from video feed\n",
    "# (returns float which we need to convert to integer for later on!)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Keep in the loop until you press \"q\"\n",
    "while True:\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret: # ret is True when frame is captured\n",
    "        # Convert color image to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('frame',gray)\n",
    "\n",
    "        # This command let's us quit with the \"q\" button on a keyboard.\n",
    "        # Simply pressing X on the window won't work!\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        print('- Webcam not found...')\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture and destroy the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (EXTRA) Writing to video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Automatically grab width and height from video feed\n",
    "# (returns float which we need to convert to integer for later on!)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "\n",
    "# MACOS AND LINUX: *'XVID' (MacOS users may want to try VIDX as well just in case)\n",
    "# WINDOWS *'VIDX'\n",
    "writer = cv2.VideoWriter('testvideo.mp4', cv2.VideoWriter_fourcc(*'XVID'),25, (width, height))\n",
    "\n",
    "## This loop keeps recording until you hit Q or escape the window\n",
    "## You may want to instead use some sort of timer, like from time import sleep and then just record for 5 seconds.\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    \n",
    "    # Write the video\n",
    "    writer.write(frame)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    \n",
    "    # This command let's us quit with the \"q\" button on a keyboard.\n",
    "    # Simply pressing X on the window won't work!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "        \n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (EXTRA) Opening and showing video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "# Same command function as streaming, its just now we pass in the file path, nice!\n",
    "cap = cv2.VideoCapture('filename.mp4') \n",
    "\n",
    "\n",
    "# Always a good idea to check if the video was acutally there\n",
    "# If you get an error at thsi step, triple check your file path!!\n",
    "if cap.isOpened()== False: \n",
    "    print(\"Error opening the video file. Please double check your file path for typos. Or move the movie file to the same location as this script/notebook\")\n",
    "    \n",
    "\n",
    "# While the video is opened\n",
    "while cap.isOpened():\n",
    "    \n",
    "    # Read the video file.\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # If we got frames, show them.\n",
    "    if ret == True:\n",
    "        # Display the frame at same frame rate of recording\n",
    "        # Watch lecture video for full explanation\n",
    "        \n",
    "        # FRAMES PER SECOND FOR VIDEO\n",
    "        #fps = 25\n",
    "        #time.sleep(1/fps) ## TO WATCH IN A MORE REAL TIME!\n",
    "        \n",
    "        cv2.imshow('frame',frame)\n",
    " \n",
    "        # Press q to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            \n",
    "            break\n",
    " \n",
    "    # Or automatically break this whole loop if the video is over.\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "# Closes all the frames\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE: Write a script to detect faces from the webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) \n",
    "\n",
    "while True: \n",
    "    \n",
    "    ret, frame = cap.read(0) \n",
    "     \n",
    "    frame = detect_face(frame)\n",
    " \n",
    "    cv2.imshow('Video Face Detection', frame) \n",
    " \n",
    "    c = cv2.waitKey(1) \n",
    "    if c == 27: \n",
    "        break \n",
    "        \n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Facial Landmarks from Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import dlib # Package used for the face landmark detection\n",
    "import cv2\n",
    " \n",
    "# let's code a face detector(HOG) and after detect the \n",
    "# landmarks on this detected face\n",
    "\n",
    "# p = our pre-treined model directory, on my case, it's on the same script's diretory.\n",
    "p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "# Use the face detector from dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(p)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    " \n",
    "while True:\n",
    "    # Getting out image by webcam \n",
    "    _, image = cap.read()\n",
    "    # Converting the image to gray scale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    # Get faces into webcam's image\n",
    "    rects = detector(gray, 0)\n",
    "    \n",
    "    # For each detected face, find the landmark.\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # Make the prediction and transfom it to numpy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "        # Draw on our image, all the finded cordinate points (x,y) \n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "    \n",
    "    # Show the image\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF # press ESC to exit\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a face recognition (60 min)\n",
    "    - Define a function to compute the Euclidean distance between points\n",
    "    - Define your metrics from the face landmarks\n",
    "    - Write a function that computes the face metrics given a face image\n",
    "    - Training: write a function that read images from the webcam, computes the facemetrics and store them in a database\n",
    "    - Write a function that given a face image and a database, returns the average distance of the k-closest face metrics (kNN)\n",
    "    - Write a function that reads the webcam, get the face image and returns how close it is from the database.\n",
    "- [ ] Run object detection and tracking live demo (15 min)\n",
    "- Conclusions (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Face Metric and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Compute the euclidean distance between 2 points\n",
    "def edist(xy1,xy2):\n",
    "    return np.sqrt((xy1[0]-xy2[0])*(xy1[0]-xy2[0])+(xy1[1]-xy2[1])*(xy1[1]-xy2[1]))\n",
    "1\n",
    "\n",
    "# Compute the Euclidean distance between two points (N-dimensional)\n",
    "def edistNdim(xy1,xy2):\n",
    "    s = 0\n",
    "    for i in range(len(xy1)):\n",
    "        s += (xy1[i]-xy2[i])*(xy1[i]-xy2[i])\n",
    "    return np.sqrt(s)\n",
    "\n",
    "# Given a set of points (xyset), \n",
    "# compute the average distance of a point xy to this set \n",
    "def edist2set(xy,xyset):\n",
    "    dist = []\n",
    "    for i in range(len(xyset)):\n",
    "        #print('- Point:',i)\n",
    "        dist.append(edistNdim(xy,xyset[i]))\n",
    "    return np.mean(dist)\n",
    "\n",
    "# Get the average distance of the k-closest points from xyset\n",
    "def knn_dist(xy,xyset,k):\n",
    "    dist = []\n",
    "    for i in range(len(xyset)):\n",
    "        #print('- Point:',i)\n",
    "        dist.append(edistNdim(xy,xyset[i]))\n",
    "    \n",
    "    return np.mean(np.sort(dist)[:k])\n",
    "\n",
    "# Get the middle point\n",
    "def mpoint(xy1,xy2):\n",
    "    return (xy1[0]+xy2[0])/2,(xy1[1]+xy2[1])/2\n",
    "\n",
    "# Define a set of face metrics that you think it might be useful \n",
    "def get_face_metric2(shape):\n",
    "    face_dist = []\n",
    "    # key positions\n",
    "    eyeL = mpoint(shape[36],shape[39])\n",
    "    eyeR = mpoint(shape[42],shape[45])\n",
    "    eyeCenter = mpoint(eyeL,eyeR)\n",
    "    lip = mpoint(shape[62],shape[66])\n",
    "    eyebrow = mpoint(shape[19],shape[24])\n",
    "    facebase = mpoint(shape[4],shape[12])\n",
    "    # normalization factor\n",
    "    face_width = edist(shape[0],shape[16])\n",
    "    faceb_width = edist(shape[4],shape[12])\n",
    "    lip_width = edist(shape[48],shape[54])\n",
    "    # Distances\n",
    "    nose_width = edist(shape[31],shape[35])\n",
    "    nose_hight = edist(shape[27],shape[33])\n",
    "    humor_dist = (facebase[1]-lip[1])/lip_width\n",
    "    \n",
    "    e2chin_dist = edist(eyeCenter,shape[8])/face_width\n",
    "    e2e_dist = edist(eyeL,eyeR)/face_width\n",
    "    e2nose = edist(eyeCenter,shape[33])/face_width\n",
    "    eyebrow_dist = edist(shape[19],shape[24])/face_width\n",
    "    e2lip = edist(eyeCenter,lip)/face_width\n",
    "    noseratio = nose_width / nose_hight\n",
    "    faceBratio = faceb_width/ face_width\n",
    "    faceCratio = edist(eyebrow,shape[8])/face_width\n",
    "    \n",
    "    eL2nose = edist(eyeL,shape[33])/face_width\n",
    "    eR2nose = edist(eyeR,shape[33])/face_width\n",
    "    lipL2nose = edist(shape[48],shape[33])/face_width\n",
    "    lipR2nose = edist(shape[54],shape[33])/face_width\n",
    "    \n",
    "    face_dist.append(e2chin_dist)\n",
    "    face_dist.append(noseratio)\n",
    "    face_dist.append(faceBratio)\n",
    "    face_dist.append(faceCratio)\n",
    "    face_dist.append(humor_dist)\n",
    "\n",
    "    face_dist.append(e2e_dist)\n",
    "    face_dist.append(e2nose)\n",
    "    #face_dist.append(eyebrow_dist)\n",
    "    face_dist.append(e2lip)\n",
    "    \n",
    "    face_dist.append(eL2nose)\n",
    "    face_dist.append(eR2nose)\n",
    "    face_dist.append(lipL2nose)\n",
    "    face_dist.append(lipR2nose)\n",
    "    \n",
    "    return face_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to collect face images from Webcam (0) or a movie file and return the set of face metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    " \n",
    "\n",
    "def movie2facemetrics(clipname): # \"0\" will lead to the webcam!!\n",
    "    # Timer\n",
    "    tic = time.time()\n",
    "    # Store all landmarks\n",
    "    alllandmarks = []\n",
    "    \n",
    "    # Define the face detectors\n",
    "    # p = our pre-treined model directory, on my case, it's on the same script's diretory.\n",
    "    p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(p)\n",
    "\n",
    "    cap = cv2.VideoCapture(clipname)\n",
    "\n",
    "    while True:\n",
    "        # Getting out image by webcam \n",
    "        ret, image = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # Converting the image to gray scale\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Get faces into webcam's image\n",
    "            rects = detector(gray, 0)\n",
    "\n",
    "            # For each detected face, find the landmark.\n",
    "            for (i, rect) in enumerate(rects):\n",
    "                # Make the prediction and transfom it to numpy array\n",
    "                shape = predictor(gray, rect)\n",
    "                shape = face_utils.shape_to_np(shape)\n",
    "                # Metrics\n",
    "                landmarks = get_face_metric2(shape)\n",
    "                # Draw on our image, all the finded cordinate points (x,y) \n",
    "                for i,(x, y) in enumerate(shape):\n",
    "                    if i in [0,8,16,36,39,42,45,30,48,54, 4,12,19,24]:             \n",
    "                        cv2.putText(image,\"%i\"%(i+1), (x,y), cv2.FONT_HERSHEY_SIMPLEX, 0.35, 255)\n",
    "                    #else:\n",
    "                    #    cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "                (x,y) = shape[8]\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                cv2.putText(image,'%.2f,%.2f,%.2f,%.2f,%.2f'%(landmarks[0],landmarks[1],landmarks[2],landmarks[3],landmarks[4]),(x-150,y+20), font, 1, (200,0,0), 3, cv2.LINE_AA)\n",
    "                #print(landmarks)\n",
    "                #variable = input('(PRESS any key to continue...)')\n",
    "\n",
    "                # Time\n",
    "                cv2.putText(image,\"%d sec\"%(time.time()-tic), (10,30),font,1, 255,3)\n",
    "\n",
    "                # Absolute face size approx\n",
    "                absE2chin = edist(shape[26],shape[8])\n",
    "                # Draw rectangle if close\n",
    "                if absE2chin > 180.0:\n",
    "                    LeftUp = (shape[0][0],shape[19][1]-10)\n",
    "                    RightDown = (shape[16][0],shape[8][1])\n",
    "                    cv2.rectangle(image,pt1=LeftUp,pt2=RightDown,color=(0,255,0),thickness=2)\n",
    "                    # Store landmarks if close enough\n",
    "                    alllandmarks.append(landmarks)\n",
    "\n",
    "            # Show the image\n",
    "            cv2.imshow(\"Output\", image)\n",
    "\n",
    "            k = cv2.waitKey(5) & 0xFF\n",
    "            if k == 27:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    \n",
    "    return alllandmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the face detector (collect face metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a dictionary into a pickle file.\n",
    "import pickle\n",
    "\n",
    "allland_jaime = movie2facemetrics(0)\n",
    "# Save the landmarks just in case the notebook crashes...\n",
    "pickle.dump(allland_jaime, open( \"allland_jaime7.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Recognition using Trained Landmarks + kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "from collections import deque\n",
    "import dlib\n",
    "import cv2\n",
    "import time\n",
    "import os\n",
    " \n",
    "# Vamos inicializar um detector de faces (HOG) para então\n",
    "# let's go code an faces detector(HOG) and after detect the \n",
    "# landmarks on this detected face\n",
    "\n",
    "def recognize_face(clipname,allland1,label1,allland2,label2):\n",
    "    alllandmarks = []\n",
    "    \n",
    "    # p = our pre-treined model directory, on my case, it's on the same script's diretory.\n",
    "    p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(p)\n",
    "\n",
    "    cap = cv2.VideoCapture(clipname)\n",
    "\n",
    "    iframe = 0\n",
    "    urdist1 = deque(maxlen=40)\n",
    "    urdist2 = deque(maxlen=40)\n",
    "    mytext = ''\n",
    "    absE2chin = 0\n",
    "    tic = time.time()\n",
    "    while True:\n",
    "        iframe+=1\n",
    "        # Getting out image by webcam \n",
    "        _, image = cap.read()\n",
    "        # Converting the image to gray scale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Get faces into webcam's image\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        # For each detected face, find the landmark.\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            \n",
    "            # Make the prediction and transfom it to numpy array\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "            # Metrics\n",
    "            landmarks = get_face_metric2(shape)\n",
    "            \n",
    "            # Draw on our image, all the finded cordinate points (x,y) \n",
    "            #for i,(x, y) in enumerate(shape):\n",
    "                #if i in [0,8,16,36,39,42,45,30,48,54, 4,12,19,24]:             \n",
    "                #    cv2.putText(image,\"%i\"%(i+1), (x,y), cv2.FONT_HERSHEY_SIMPLEX, 0.35, 255)\n",
    "                \n",
    "            # Distance to allland\n",
    "            urdistA = knn_dist(landmarks,allland1,5)\n",
    "            urdistB = knn_dist(landmarks,allland2,5)\n",
    "            urdist1.append(urdistA) # moving window of size 10\n",
    "            urdist2.append(urdistB)\n",
    "    \n",
    "            aux = np.random.rand()\n",
    "            # Absolute face size approx\n",
    "            absE2chin = edist(shape[26],shape[8])\n",
    "            # Draw rectangle if close\n",
    "            if absE2chin > 180.0:\n",
    "                    LeftUp = (shape[0][0],shape[19][1]-10)\n",
    "                    RightDown = (shape[16][0],shape[8][1])\n",
    "                    cv2.rectangle(image,pt1=LeftUp,pt2=RightDown,color=(0,255,0),thickness=2)\n",
    "            # Recognize every 2 sec\n",
    "            if (time.time()-tic)>2:\n",
    "                if absE2chin > 180.0:\n",
    "                    dist1 = np.array(urdist1).mean()\n",
    "                    dist2 = np.array(urdist2).mean()\n",
    "                    if (dist1 < 0.12) & (dist1<dist2):\n",
    "                        tic = time.time()\n",
    "                        mytext = label1\n",
    "                    elif (dist2 < 0.12) & (dist2<dist1):\n",
    "                        tic = time.time()\n",
    "                        mytext = label2\n",
    "                    elif aux <0.5:\n",
    "                        tic = time.time()\n",
    "                        mytext = 'Camper!'\n",
    "                    else:\n",
    "                        tic = time.time()\n",
    "                        mytext = '???'\n",
    "                else:\n",
    "                    tic = time.time()\n",
    "                    mytext = 'too far...'\n",
    "            \n",
    "            # Get coordinate to write text\n",
    "            (x,y) = shape[8]\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(image,'1: %.2f, 2: %.2f'%(urdistA,urdistB),(x-50,y+50), font, 0.7, (0,200,0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(image,'%s'%mytext,(x-20,y+20), font, 1, (0,200,0), 3, cv2.LINE_AA)\n",
    "            \n",
    "            # Time\n",
    "            cv2.putText(image,\"%d sec\"%(time.time()-tic), (10,30),font,0.7, (0,200,0),2)\n",
    "            cv2.putText(image,\"%.2f\"%(absE2chin), (10,60),font,0.7, (0,200,0),2)\n",
    "            \n",
    "            # Store landmarks\n",
    "            alllandmarks.append(landmarks)\n",
    "\n",
    "        # Show the image\n",
    "        cv2.imshow(\"Output\", image)\n",
    "\n",
    "        k = cv2.waitKey(5) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()\n",
    "    \n",
    "    return alllandmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "allland_jaime1 = pickle.load( open( \"allland_jaime.p\", \"rb\" ) )\n",
    "allland_jaime2 = pickle.load( open( \"allland_jaime2.p\", \"rb\" ) )\n",
    "allland_jaime3 = pickle.load( open( \"allland_jaime3.p\", \"rb\" ) )\n",
    "allland_jaime3 = pickle.load( open( \"allland_jaime4.p\", \"rb\" ) )\n",
    "allland_jaime3 = pickle.load( open( \"allland_jaime5.p\", \"rb\" ) )\n",
    "allland_andrey = pickle.load( open( \"allland_andrey.p\", \"rb\" ) )\n",
    "allland_andrey1 = pickle.load( open( \"allland_andrey1.p\", \"rb\" ) )\n",
    "allland_david = pickle.load( open( \"allland_david.p\", \"rb\" ) )\n",
    "allland_ben = pickle.load( open( \"allland_ben.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = recognize_face(0,allland_jaime2,'Jaime',allland_david,'David')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (EXTRA) Making audio files to make it fun\n",
    "Combine with your face recognition function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required module for text \n",
    "# to speech conversion \n",
    "from gtts import gTTS \n",
    "\n",
    "# This module is imported so that we can \n",
    "# play the converted audio \n",
    "import os \n",
    "\n",
    "# The text that you want to convert to audio \n",
    "mytext = 'Hey, Ben how did you like Sigma Camp so far?'\n",
    "language = 'en'\n",
    "#language = 'fr'\n",
    "\n",
    "label = \"ben\"\n",
    "\n",
    "# Passing the text and language to the engine, \n",
    "# here we have marked slow=False. Which tells \n",
    "# the module that the converted audio should \n",
    "# have a high speed \n",
    "myobj = gTTS(text=mytext, lang=language, slow=False) \n",
    "\n",
    "# Saving\n",
    "myobj.save(\"%s.mp3\"%label)\n",
    "\n",
    "# Playing the converted file \n",
    "os.system(\"%s.mp3\"%label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
